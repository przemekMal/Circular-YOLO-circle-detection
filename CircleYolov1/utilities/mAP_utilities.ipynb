{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMy56GzV9Gey32SGsmoRcTK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Inhc3--izaBV"},"outputs":[],"source":["def non_max_suppression(apples, iou_threshold=0.5, threshold = 0.5):\n","    \"\"\"\n","    Performs Non-Maximum Suppression (NMS) on a list of detected objects.\n","\n","    Args:\n","        detections (list): A list of detected objects, where each object is represented as a tuple.\n","                           Each tuple consists of class index, confidence score, and bounding box coordinates.\n","        iou_threshold (float): Intersection over Union (IoU) threshold for considering two bounding boxes as the same object.\n","                               Default is 0.5.\n","        threshold (float): Confidence score threshold for filtering detections. Default is 0.5.\n","\n","    Returns:\n","        list: List of selected detections after Non-Maximum Suppression.\n","    \"\"\"\n","    assert type(apples) == list\n","\n","    # Filter detections based on confidence threshold\n","    apples = [apple for apple in apples if apple[1]>threshold]\n","\n","    # Sort detections by confidence score in descending order\n","    apples = sorted(apples, key=lambda x: x[1], reverse=True)\n","    apples_to_return = []\n","    while apples:\n","        chosen_apple = apples.pop(0)\n","        apples = [\n","            apple\n","            for apple in apples\n","            if apple[0] != chosen_apple[0] # Different class\n","            or intersection_over_union(\n","                torch.tensor(chosen_apple[2:]),\n","                torch.tensor(apple[2:])\n","            )\n","            < iou_threshold\n","        ]\n","        apples_to_return.append(chosen_apple)\n","    return apples_to_return\n","\n","\n","def convert_grid_boxes(predictions, S=7, C=1):\n","    \"\"\"\n","    Convert grid-based bounding box predictions to entire image ratio.\n","\n","    Args:\n","        predictions (torch.Tensor): Tensor of bounding box predictions, with shape [batch_size, S, S, -1].\n","                                     Each prediction consists of class probabilities, bounding box coordinates (x, y, r), where r represent radius.\n","        S (int): Size of the grid. Default is 7.\n","        C (int): Number of classes. Default is 1.\n","\n","    Returns:\n","        torch.Tensor: Converted bounding box predictions with shape [batch_size, S, S, -1], where each prediction is represented as (class_index, confidence, x, y, r).\n","    \"\"\"\n","    predictions = predictions.to('cpu') # Move predictions to CPU\n","    batch_size = predictions.shape[0] # Get batch size\n","    predictions = predictions.reshape(batch_size, S, S, -1) # Reshape predictions to [batch_size, S, S, -1]\n","\n","    # Determine the number of bounding box predictions per grid cell\n","    shape_size_loop = int((predictions.shape[-1] - C)/4)\n","\n","    # Iterate over each bounding box prediction\n","    for bbox in range(1, shape_size_loop, 1):\n","        # Create a mask to select bounding box predictions with higher confidence scores\n","        mask = predictions[..., C:(C+1)] < predictions[..., (C+4*bbox):(C+1+4*bbox)]\n","        # Create a slice to select the current bounding box predictions\n","        bbox_slice = slice(C + 4 * bbox, C + 4 * bbox + 4)\n","        # Update bounding box predictions with higher confidence scores\n","        predictions[..., C:(C+4)] = torch.where(mask, predictions[..., bbox_slice], predictions[..., C:(C+4)])\n","\n","    # Extract class probabilities, bounding box coordinates, and radius from predictions\n","    prob = (predictions[...,C:(C+1)])\n","    cell_indicies = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n","    x = 1/S * (predictions[...,(C+1):(C+2)] + cell_indicies)\n","    y = 1/S * (predictions[...,(C+2):(C+3)] + cell_indicies.permute(0, 2, 1, 3))\n","    r = predictions[..., (C+3):(C+4)]\n","\n","    # Determine class indices based on the highest probability\n","    if C < 2:\n","        class_idx = torch.zeros_like(r)\n","    else:\n","        class_idx = predictions[..., 0:C].argmax(-1).unsqueeze(-1)\n","\n","    # Concatenate class index, confidence, x, y, and radius to form converted bounding box predictions\n","    converted_boxes = torch.cat((class_idx, prob, x, y, r), dim=-1)\n","    return converted_boxes\n","\n","\n","def get_bboxes(model, loader, IoU_threshold: float = 0.5, threshold: float = 0.5, S: int = 7, C: int = 1):\n","    \"\"\"\n","    Extracts bounding boxes from model predictions and true labels.\n","\n","    Args:\n","        model (torch.nn.Module): Trained model for predicting bounding boxes.\n","        loader (torch.utils.data.DataLoader): DataLoader containing images and corresponding labels.\n","        IoU_threshold (float): Intersection over Union threshold for non-maximum suppression. Default is 0.5.\n","        threshold (float): Threshold for filtering out low confidence predictions. Default is 0.4.\n","        S (int): Size of the grid used in the model. Default is 7.\n","        C (int): Number of classes. Default is 1.\n","\n","    Returns:\n","        Tuple: A tuple containing lists of predicted and true bounding boxes respectively.\n","               Each bounding box is represented as [image_index, class_index, confidence, x, y, r].\n","    \"\"\"\n","    all_pred_box = [] # List to store predicted bounding boxes\n","    all_true_box = [] # List to store true bounding boxe\n","    train_idx = 0 # Index for tracking the image\n","    model.eval() # Set model to evaluation mode\n","\n","    for batch_idx, (x, labels) in enumerate(loader):\n","        x, labels = x.to(device), labels.to(device) # Move data to device\n","        with torch.no_grad():\n","            predictions = model(x) # Forward pass\n","\n","        # Convert predictions and labels to individual bounding boxes\n","        bboxes = grid_boxes_to_boxes(predictions, S=S, C=C)\n","        true_labels = grid_boxes_to_boxes(labels, S=S, C=C)\n","        batch_size = x.shape[0]\n","\n","        # Iterate over each image in the batch\n","        for idx in range(batch_size):\n","            # Apply non-maximum suppression to predictions\n","            nms_predictions = non_max_suppression(bboxes[idx], IoU_threshold, threshold)\n","\n","            # Append predicted bounding boxes to the list\n","            for pred in nms_predictions:\n","                all_pred_box.append([train_idx] + pred)\n","\n","            # Append true bounding boxes to the list\n","            for true_leb in true_labels[idx]:\n","                if true_leb[1] > threshold:\n","                    all_true_box.append([train_idx] + true_leb)\n","\n","            train_idx += 1 # Increment image index\n","\n","    model.train() # Set model back to training mode\n","    return all_pred_box, all_true_box # Return lists of predicted and true bounding boxes\n","\n","\n","import torch\n","from collections import Counter\n","\n","#mAP@0.5:0.1:0.95\n","def mean_average_precision(\n","    pred_boxes,               # List of predicted bounding boxes in format (img_idx, class, probability, x, y, r)\n","    true_boxes,               # List of true bounding boxes in format (img_idx, class, probability, x, y, r)\n","    threshold_mAP=0.5,        # Initial IoU threshold for mAP calculations. Default: 0.5\n","    step_threshold=0.05,      # Step size for increasing the threshold in mAP calculations. Default: 0.05\n","    stop_threshold_mAP=0.95,  # Threshold at which to stop mAP calculations. Default: 0.95k\n","    C=1,                      # Number of classes. Default: 1\n","    epsilon=1e-12             # Small additional value in the denominator to avoid division by zero. Default: 1e-12\n","    ):\n","    \"\"\"\n","    Calculates the mean average precision for a set of predicted and true bounding boxes.\n","\n","    Args:\n","        pred_boxes (list): List of predicted bounding boxes.\n","        true_boxes (list): List of true bounding boxes.\n","        threshold_mAP (float): Initial IoU threshold for mAP calculations. Default: 0.5.\n","        step_threshold (float): Step size for increasing the threshold in mAP calculations. Default: 0.05.\n","        stop_threshold_mAP (float): Threshold at which to stop mAP calculations. Default: 0.95.\n","        C (int): Number of classes. Default: 1.\n","        epsilon (float): Small additional value in the denominator to avoid division by zero. Default: 1e-12.\n","\n","    Returns:\n","        float: Mean average precision (mAP).\n","    \"\"\"\n","    mean_average_precision = [] # Table of values mAP for each step\n","\n","    # Iterating calulate mAP while threshold is lower than end case\n","    while threshold_mAP < stop_threshold_mAP:\n","        average_precision = [] # List of Average Precision for current threshold step\n","        # Calculate Average Precision for each class\n","        for c in range(C):\n","            detection_list = []\n","            ground_truhts = []\n","            for detection in pred_boxes:\n","                if detection[1] == c:\n","                    detection_list.append(detection)\n","            for true_box in true_boxes:\n","                if true_box[1] == c:\n","                    ground_truhts.append(true_box)\n","            #Making dic, wich reprezented how many bbox, imgs have\n","            #amount_boxes = {0:3, 1:5, 2:3}\n","            amount_bboxes = Counter([gt[0] for gt in ground_truhts])\n","\n","            #changing dir to had val = zero tensors, with shape reprezented num of imgs\n","            #amount_boxes = {0:torch.zeros([0,0,0]), 1:torch.zeros([0,0,0,0,0]), 2:torch.zeros([0,0,0])}\n","            for key, val in amount_bboxes.items():\n","                amount_bboxes[key]= torch.zeros(val)\n","            #sort over probability\n","            detection_list.sort(key= lambda x: x[2], reverse=True)\n","            TP = torch.zeros((len(detection_list)))\n","            FP = torch.zeros((len(detection_list)))\n","            total_true_boxes = len(ground_truhts)\n","\n","            for detection_idx, detection in enumerate(detection_list):\n","              #Taking only bboxs for correct img\n","                ground_truth_img = [ bbox for bbox in ground_truhts if bbox[0] == detection[0] ]\n","              #number of target bbox\n","                num_gts = len(ground_truth_img)\n","                best_iou = 0\n","                for idx, gt in enumerate(ground_truth_img):\n","                    iou = intersection_over_union(torch.tensor(gt[3:]), torch.tensor(detection[3:]))\n","                    if iou > best_iou:\n","                        best_iou = iou\n","                        best_gt_idx = idx\n","                if best_iou > threshold_mAP:\n","                    if amount_bboxes[detection[0]][best_gt_idx] == 0:\n","                        TP[detection_idx] = 1\n","                        amount_bboxes[detection[0]][best_gt_idx] = 1\n","                    else:\n","                        FP[detection_idx] = 1\n","                else:\n","                    FP[detection_idx] = 1\n","            #cumulative sum for recall [1, 1, 0, 1, 0] -> [1, 2, 2, 3, 3]\n","            TP_cumsum = torch.cumsum(TP, dim=0)\n","            FP_cumsum = torch.cumsum(FP, dim=0)\n","            recalls = TP_cumsum / (total_true_boxes+epsilon)\n","            precision = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n","            precision = torch.cat((torch.tensor([1]), precision))\n","            recalls = torch.cat((torch.tensor([0]), recalls))\n","            average_precision.append(torch.trapz(precision, recalls))\n","\n","        mean_average_precision.append(sum(average_precision)/len(average_precision))\n","        threshold_mAP += step_threshold\n","\n","    return sum(mean_average_precision)/len(mean_average_precision)"]}]}