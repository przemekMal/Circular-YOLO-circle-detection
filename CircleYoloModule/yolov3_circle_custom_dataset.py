# -*- coding: utf-8 -*-
"""yoloV3_circle_custom_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GM3ZXxfptJLHz7b6fSDubHkY-4hvYhqs
"""
import torch
import torch.nn as nn
from pandas import read_csv
from torch.utils.data import Dataset
from torchvision.io import read_image, ImageReadMode
import math
import numpy as np
import os


def iou_radius(boxes1, boxes2):
    """
    Parameters:
        boxes1 (tensor): radius of the first bounding boxes
        boxes2 (tensor): radius of the second bounding boxes
    Returns:
        tensor: Intersection over union of the corresponding boxes
    """
    intersection =  torch.min(boxes1[..., 0], boxes2[..., 0]) **2 #*pi() >>> A
    union = torch.max(boxes1[..., 0], boxes2[..., 0]) **2 #*pi() >>> B

    return intersection / union

class Datasets(Dataset):
  def __init__(
      self, csv_file,
      img_dir, label_dir,
      anchors, #pudeÅ‚ka kotwicowe o ustalonej wielkosci
      transform = None,
      S=[13, 26, 52],
      C=1, B=1 #C - class, B-num boxes
  ):
    self.annotations = read_csv(csv_file)
    self.img_dir = img_dir
    self.label_dir = label_dir
    self.transform = transform
    self.S = S
    self.anchors = anchors #for all 3 scales >>> [[(0.8), (0.6), (0.5)] + [(0.15), (0.1), (0.09)] + [(0.05), (0.033), (0.02)]]
    self.num_anchors = torch.tensor([value for anchor in self.anchors for value in anchor]).shape[0] #Nine in total cuz: 3_anchors x 3_scales
    self.num_anchors_per_scale = self.num_anchors // 3 #cuz 3 scales: 13x13, 26x26, 52x52
    self.C = C
    self.ignore_iou_threshold = 0.5

  def __len__(self):
    return len(self.annotations)

  def __getitem__(self, index):
    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])
    
    with open(label_path, 'r') as file:
        lines = file.readlines()  # Wczytaj linie z pliku
    lines_array = np.array([[float(value) for value in line.split()] for line in lines if line.strip()])

    label_matrix = [torch.zeros((self.num_anchors_per_scale, S, S, 5)) for S in self.S] #[p, x, y, r, c]
    for box in lines_array:
      iou_anchors = iou_radius(torch.tensor(box[3:4]).repeat(self.num_anchors, 1), self.anchors) #which anchor and whith cell should be responsible for prediction in all 3 diff scales
      anchor_indicies = iou_anchors.argsort(descending=True, dim=0) # example: [0.1406, 1.0000, 0.3787] >>> [1, 2, 0]

      class_label, x, y, r, = box
      has_anchor = [False] * self.num_anchors #make sure there is bbox ancher for each of 3 scales (all True in the end)
      for anchor_idx in anchor_indicies:
          scale_idx = anchor_idx // self.num_anchors_per_scale #Whitch scale to choose. Example: 0, 1, 2
          anchor_on_scale = anchor_idx % self.num_anchors_per_scale #Whitch anchor to choose Example: 0, 1, 2
          S = self.S[scale_idx] #How many cells we have
          i, j = math.floor(S * y), math.floor(S * x)# i,j represents the cell row and cell column
          x_cell, y_cell = S*x-j, S*y-i
          r_for_cell = r*S
            #Cheking if (x or y > 1) or (x or y < 0) for save in corect grid
          if x >= 1:
              j=S-1
              x_cell+=math.ceil((x-1)*S)
          elif x < 0:
              j=0
              x_cell-=math.ceil(-x*S)
          if y >= 1:
              i=S-1
              y_cell+=math.ceil((y-1)*S)
          elif y < 0:
              i=0
              y_cell-=math.ceil(-y*S)
              
          anchor_taken = label_matrix[scale_idx][anchor_on_scale, i, j, 0]

          if not anchor_taken and not has_anchor[scale_idx]:
              box_cord = torch.tensor([x_cell, y_cell, r_for_cell])
              #label_matrix[scale_idx][anchor_on_scale, i, j, 0] = 1
              #label_matrix[scale_idx][anchor_on_scale, i, j, 1:4] = box_cord
              #label_matrix[scale_idx][anchor_on_scale, i, j, 4] = int(class_label)
              label_matrix[scale_idx][anchor_on_scale, i, j, :]  = torch.tensor([1] + [x for x in box_cord] + [int(class_label)])
              has_anchor[scale_idx] = True

          elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_threshold:
              label_matrix[scale_idx][anchor_on_scale, i, j, 0] = -1 #ignore prediction

    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])
    image = read_image(img_path, mode = ImageReadMode.RGB)
    if self.transform:
      image = self.transform(image)

    return image, tuple(label_matrix)